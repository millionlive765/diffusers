{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"XU7NuMAA2drw","outputId":"7eb9b063-664f-4a42-e960-728ec9608c42"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tesla T4, 15109 MiB, 15109 MiB\n"]}],"source":["#@markdown Check type of GPU and VRAM available.\n","!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"]},{"cell_type":"markdown","metadata":{"id":"BzM7j0ZSc_9c"},"source":["https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth"]},{"cell_type":"markdown","metadata":{"id":"wnTMyW41cC1E"},"source":["## Install Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLWXPZqjsZVV"},"outputs":[],"source":["!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n","!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n","%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n","%pip install -q -U --pre triton\n","%pip install -q accelerate==0.12.0 transformers ftfy bitsandbytes gradio natsort"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"y4lqqWT_uxD2"},"outputs":[],"source":["#@title Login to HuggingFace ðŸ¤—\n","\n","#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work.\n","# https://huggingface.co/settings/tokens\n","!mkdir -p ~/.huggingface\n","HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n","!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"]},{"cell_type":"markdown","metadata":{"id":"XfTlc8Mqb8iH"},"source":["### Install xformers from precompiled wheel."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6dcjPnnaiCn","outputId":"ac7dc3db-27a2-4dd4-963e-4d77dc313a5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102.9 MB 115 kB/s \n","\u001b[?25h"]}],"source":["%pip install -q https://github.com/ShivamShrirao/xformers-wheels/releases/download/4c06c79/xformers-0.0.15.dev0+4c06c79.d20221201-cp38-cp38-linux_x86_64.whl\n","# These were compiled on Tesla T4, should also work on P100.\n","\n","# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n","# %pip install git+https://github.com/facebookresearch/xformers@4c06c79#egg=xformers"]},{"cell_type":"markdown","metadata":{"id":"G0NV324ZcL9L"},"source":["## Settings and run"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Rxg0y5MBudmd"},"outputs":[],"source":["#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n","save_to_gdrive = True #@param {type:\"boolean\"}\n","if save_to_gdrive:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","#@markdown Name/Path of the initial model.\n","MODEL_NAME = \"millionlive765/ntest\" #@param {type:\"string\"}\n","\n","#@markdown Enter the directory name to save model at.\n","\n","OUTPUT_DIR = \"stable_diffusion_weights/hor\" #@param {type:\"string\"}\n","if save_to_gdrive:\n","    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n","else:\n","    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n","\n","print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n","\n","!mkdir -p $OUTPUT_DIR"]},{"cell_type":"markdown","metadata":{"id":"qn5ILIyDJIcX"},"source":["# Start Training\n","\n","Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n","\n","\n","| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n","| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n","| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n","| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n","| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n","| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n","| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n","| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n","| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n","| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n","| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"]},{"cell_type":"markdown","metadata":{"id":"-ioxxvHoicPs"},"source":["Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n","\n","remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n","\n","remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vDpCxId1aCm"},"outputs":[],"source":["# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n","\n","concepts_list = [\n","    {\n","        \"instance_prompt\":      \"photo of a hor girl\",\n","        \"class_prompt\":         \"photo of a girl\",\n","        \"instance_data_dir\":    \"/content/drive/MyDrive/data/hor\",\n","        \"class_data_dir\":       \"/content/drive/MyDrive/data/girl\"\n","    },\n","#     {\n","#         \"instance_prompt\":      \"photo of ukj person\",\n","#         \"class_prompt\":         \"photo of a person\",\n","#         \"instance_data_dir\":    \"/content/data/ukj\",\n","#         \"class_data_dir\":       \"/content/data/person\"\n","#     }\n","]\n","\n","# `class_data_dir` contains regularization images\n","import json\n","import os\n","for c in concepts_list:\n","    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n","\n","with open(\"concepts_list.json\", \"w\") as f:\n","    json.dump(concepts_list, f, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"32gYIDDR1aCp"},"outputs":[],"source":["#@markdown Upload your images by running this cell.\n","\n","#@markdown OR\n","\n","#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster)\n","\n","import os\n","from google.colab import files\n","import shutil\n","\n","for c in concepts_list:\n","    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n","    uploaded = files.upload()\n","    for filename in uploaded.keys():\n","        dst_path = os.path.join(c['instance_data_dir'], filename)\n","        shutil.move(filename, dst_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjcSXTp-u-Eg"},"outputs":[],"source":["!accelerate launch train_dreambooth.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --with_prior_preservation --prior_loss_weight=1.0 \\\n","  --seed=1337 \\\n","  --resolution=512 \\\n","  --train_batch_size=1 \\\n","  --mixed_precision=\"fp16\" \\\n","  --use_8bit_adam \\\n","  --gradient_accumulation_steps=1 \\\n","  --learning_rate=5e-6 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --num_class_images=200 \\\n","  --sample_batch_size=4 \\\n","  --max_train_steps=3500 \\\n","  --save_interval=10000 \\\n","  --save_sample_prompt=\"photo of a hor girl\" \\\n","  --concepts_list=\"concepts_list.json\"\n","\n","# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n","# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"89Az5NUxOWdy"},"outputs":[],"source":["#@markdown Specify the weights directory to use (leave blank for latest)\n","WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n","if WEIGHTS_DIR == \"\":\n","    from natsort import natsorted\n","    from glob import glob\n","    import os\n","    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n","print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"W8h0tglS7qlI"},"outputs":[],"source":["#@markdown Run to generate a grid of preview images from the last saved weights.\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","weights_folder = OUTPUT_DIR\n","folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n","\n","row = len(folders)\n","col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n","scale = 4\n","fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n","\n","for i, folder in enumerate(folders):\n","    folder_path = os.path.join(weights_folder, folder)\n","    image_folder = os.path.join(folder_path, \"samples\")\n","    images = [f for f in os.listdir(image_folder)]\n","    for j, image in enumerate(images):\n","        if row == 1:\n","            currAxes = axes[j]\n","        else:\n","            currAxes = axes[i, j]\n","        if i == 0:\n","            currAxes.set_title(f\"Image {j}\")\n","        if j == 0:\n","            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n","        image_path = os.path.join(image_folder, image)\n","        img = mpimg.imread(image_path)\n","        currAxes.imshow(img, cmap='gray')\n","        currAxes.axis('off')\n","        \n","plt.tight_layout()\n","plt.savefig('grid.png', dpi=72)"]},{"cell_type":"markdown","metadata":{"id":"5V8wgU0HN-Kq"},"source":["## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dcXzsUyG1aCy"},"outputs":[],"source":["#@markdown Run conversion.\n","ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n","\n","half_arg = \"\"\n","#@markdown  Whether to convert to fp16, takes half the space (2GB).\n","fp16 = False #@param {type: \"boolean\"}\n","if fp16:\n","    half_arg = \"--half\"\n","!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n","print(f\"[*] Converted ckpt saved at {ckpt_path}\")"]},{"cell_type":"markdown","metadata":{"id":"ToNG4fd_dTbF"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lJoOgLQHnC8L"},"outputs":[],"source":["#@title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n","\n","#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n","import shutil\n","from glob import glob\n","import os\n","for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n","    if f != WEIGHTS_DIR:\n","        shutil.rmtree(f)\n","        print(\"Deleted\", f)\n","for f in glob(WEIGHTS_DIR+\"/*\"):\n","    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n","        try:\n","            shutil.rmtree(f)\n","        except NotADirectoryError:\n","            continue\n","        print(\"Deleted\", f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXgi8HM4c-DA"},"outputs":[],"source":["#@title Free runtime memory\n","exit()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/ShivamShrirao/diffusers/blob/main/examples/dreambooth/DreamBooth_Stable_Diffusion.ipynb","timestamp":1670087062236}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"2.7.16"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":0}